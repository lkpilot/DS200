{"cells":[{"cell_type":"markdown","metadata":{"editable":true,"tags":[],"id":"v6BGM-Cr6Up_"},"source":["# Set up"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"MIpuDa8K6UqQ"},"outputs":[],"source":["import argparse\n","import os\n","import logging\n","import time\n","import pickle\n","from tqdm import tqdm\n","from IPython.display import display, HTML\n","\n","import torch\n","from torch.utils.data import DataLoader\n","import pytorch_lightning as pl\n","\n","from transformers import AdamW, T5ForConditionalGeneration, T5Tokenizer\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","from transformers import get_linear_schedule_with_warmup"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"VY_2o5PE6UqW"},"outputs":[],"source":["class T5FineTuner(pl.LightningModule):\n","    \"\"\"\n","    Fine tune a pre-trained T5 model\n","    \"\"\"\n","    def __init__(self, tfm_model, tokenizer):\n","        super(T5FineTuner, self).__init__()\n","        self.model = tfm_model\n","        self.tokenizer = tokenizer\n","        self.automatic_optimization = False\n","        self.validation_step_outputs = []\n","\n","    def is_logger(self):\n","        return True\n","\n","    def forward(self, input_ids, attention_mask=None, decoder_input_ids=None,\n","                decoder_attention_mask=None, labels=None):\n","        return self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            decoder_input_ids=decoder_input_ids,\n","            decoder_attention_mask=decoder_attention_mask,\n","            labels=labels,\n","        )\n","\n","    def _step(self, batch):\n","        lm_labels = batch[\"target_ids\"]\n","        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n","\n","        outputs = self(\n","            input_ids=batch[\"source_ids\"],\n","            attention_mask=batch[\"source_mask\"],\n","            labels=lm_labels,\n","            decoder_attention_mask=batch['target_mask']\n","        )\n","\n","        loss = outputs[0]\n","        return loss\n","\n","    def training_step(self, batch, batch_idx):\n","        loss = self._step(batch)\n","\n","        self.manual_backward(loss)\n","        optimizer = self.optimizers()\n","        #scheduler = self.lr_schedulers()\n","\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        #scheduler.step()\n","\n","        tensorboard_logs = {\"train_loss\": loss}\n","        return {\"loss\": loss, \"log\": tensorboard_logs}\n","\n","    def on_training_epoch_end(self, outputs):\n","        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n","        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n","        return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n","\n","    def validation_step(self, batch, batch_idx):\n","        loss = self._step(batch)\n","        self.validation_step_outputs.append(loss)\n","        return loss\n","\n","    def on_validation_epoch_end(self):\n","        avg_loss = torch.stack(self.validation_step_outputs).mean()\n","        tensorboard_logs = {\"val_loss\": avg_loss}\n","        return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n","\n","    def configure_optimizers(self):\n","        \"\"\" Prepare optimizer and schedule (linear warmup and decay) \"\"\"\n","        model = self.model\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","        optimizer = AdamW(optimizer_grouped_parameters, lr=3e-4, eps=1e-8)\n","        self.opt = optimizer\n","        return [optimizer]\n","\n","    '''\n","    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n","        #if self.trainer.use_tpu:\n","            #xm.optimizer_step(optimizer)\n","        #else:\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        self.lr_scheduler.step()\n","    '''\n","\n","    def get_tqdm_dict(self):\n","        tqdm_dict = {\"loss\": \"{:.4f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n","        return tqdm_dict\n","\n","    def train_dataloader(self):\n","        train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\")\n","        dataloader = DataLoader(train_dataset, batch_size=16,\n","                                drop_last=True, shuffle=True, num_workers=4)\n","        t_total = (\n","            (len(dataloader.dataset) // (16 * max(1, 0)))\n","            // 1\n","            * float(30)\n","        )\n","        scheduler = get_linear_schedule_with_warmup(\n","            self.opt, num_warmup_steps=0.0, num_training_steps=t_total\n","        )\n","        self.lr_scheduler = scheduler\n","        return dataloader\n","\n","    def val_dataloader(self):\n","        val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"dev\")\n","        return DataLoader(val_dataset, batch_size=16, num_workers=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8KHzgJeG6Uqa"},"outputs":[],"source":["sentiment_word_list = ['positive', 'negative', 'neutral']\n","\n","opinion2word = {'great': 'positive', 'bad': 'negative', 'ok': 'neutral'}\n","\n","opinion2word_under_o2m = {'good': 'positive', 'great': 'positive', 'best': 'positive',\n","                          'bad': 'negative', 'okay': 'neutral', 'ok': 'neutral', 'average': 'neutral'}\n","\n","numopinion2word = {'SP1': 'positive', 'SP2': 'negative', 'SP3': 'neutral'}\n","\n","aspect_cate_list = ['Candidate voice',\n","                    'Candidate flow',\n","                    'Candidate general',\n","                    'Examiner general',\n","                    'Show stage',\n","                    'Show general',\n","                    'Music',\n","                    'Others']\n","\n","def extract_spans_para(task, seq, seq_type):\n","    quads = []\n","    sents = [s.strip() for s in seq.split('[SSEP]')]\n","    if task == 'aste':\n","        for s in sents:\n","            # It is bad because editing is problem.\n","            try:\n","                c, ab = s.split(' because ')\n","                c = opinion2word.get(c[6:], 'nope')    # 'good' -> 'positive'\n","                a, b = ab.split(' is ')\n","            except ValueError:\n","                # print(f'In {seq_type} seq, cannot decode: {s}')\n","                a, b, c = '', '', ''\n","            quads.append((a, b, c))\n","    elif task == 'tasd':\n","        for s in sents:\n","            # food quality is bad because pizza is bad.\n","            try:\n","                ac_sp, at_sp = s.split(' because ')\n","\n","                ac, sp = ac_sp.split(' is ')\n","                sp = opinion2word.get(sp, 'nope')\n","                at, sp2 = at_sp.split(' is ')\n","\n","                sp = opinion2word.get(sp, 'nope')\n","                sp2 = opinion2word.get(sp2, 'nope')\n","                if sp != sp2:\n","                    print(f'Sentiment polairty of AC({sp}) and AT({sp2}) is inconsistent!')\n","\n","                # if the aspect term is implicit\n","                if at.lower() == 'it':\n","                    at = 'NULL'\n","            except ValueError:\n","                # print(f'In {seq_type} seq, cannot decode: {s}')\n","                ac, at, sp = '', '', ''\n","\n","            quads.append((ac, at, sp))\n","    elif task == 'asqp':\n","        for s in sents:\n","            # food quality is bad because pizza is over cooked.\n","            try:\n","                ac_sp, at_ot = s.split(' because ')\n","                ac, sp = ac_sp.split(' is ')\n","                sp = opinion2word.get(sp, 'nope')\n","                at, ot = at_ot.split(' is ')\n","\n","                # if the aspect term is implicit\n","                if at.lower() == 'it':\n","                    at = 'NULL'\n","            except ValueError:\n","                try:\n","                    # print(f'In {seq_type} seq, cannot decode: {s}')\n","                    pass\n","                except UnicodeEncodeError:\n","                    # print(f'In {seq_type} seq, a string cannot be decoded')\n","                    pass\n","                ac, at, sp, ot = '', '', '', ''\n","\n","            quads.append((ac, at, sp, ot))\n","    else:\n","        raise NotImplementedError\n","    return quads"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"R2kdC8i26Uqc"},"outputs":[],"source":["import pickle\n","import io\n","class CPU_Unpickler(pickle.Unpickler):\n","    def find_class(self, module, name):\n","        if module == 'torch.storage' and name == '_load_from_bytes':\n","            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n","        else:\n","            return super().find_class(module, name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6wroxffJ6Uqc"},"outputs":[],"source":["def interference(text,gen_conf):\n","  input = tokenizer.batch_encode_plus(\n","              [text], max_length=128, padding=\"max_length\",\n","              truncation=True, return_tensors=\"pt\"\n","            )\n","  outs = model.model.generate(input['input_ids'], max_length=128,generation_config=gen_conf)\n","\n","  dec = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n","  pred_list = extract_spans_para('asqp', dec[0], 'pred')\n","  return pred_list"]},{"cell_type":"markdown","metadata":{"id":"d6cR2piU6Uqd"},"source":["# Streaming"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yRiB8z7tfsMk","tags":[]},"outputs":[],"source":["import pyspark\n","from IPython.display import display, clear_output\n","from pyspark.sql import SparkSession, DataFrame\n","from pyspark.sql import functions as f\n","from pyspark.sql.functions import col\n","from pyspark.sql.functions import udf\n","from pyspark.sql.streaming import DataStreamReader\n","from pyspark.ml import PipelineModel\n","import html\n","import pandas as pd\n","from time import sleep\n","from IPython.display import display, clear_output\n","\n","pd.options.display.max_columns = None\n","pd.options.display.max_rows = 30\n","pd.options.display.max_colwidth = 150"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"7iHXjZ3v6Uqe"},"outputs":[],"source":["# SETTINGS\n","IN_PATH = \"Train.csv\"\n","timestampformat = \"EEE MMM dd HH:mm:ss zzzz yyyy\"\n","\n","scala_version='2.13'\n","spark_version='3.5.1'\n","packages = [\n","    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}'\n","    # 'org.apache.kafka:kafka-clients:3.6.0'\n","]\n","spark = SparkSession.builder.master('local').appName('kafka-example').\\\n","        config('spark.jars.packages', f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}').getOrCreate()\n","\n","# schema = spark.read.option('header',True).csv(IN_PATH).limit(10).schema\n","\n","# spark_reader = spark.read.option('header', True).schema(schema)\n","# df = spark_reader.csv(IN_PATH).coalesce(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"NKLGxSnb6Uqg"},"outputs":[],"source":["# Cần đổi topic_name giống với consumer và producer đang chạy\n","topic_name = 'NewABSA6'\n","kafka_server = 'localhost:9092'\n","\n","streamRawDf = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", kafka_server).option(\"subscribe\", topic_name).load()"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":true,"tags":[],"id":"oOYFXHfA6Uqi"},"outputs":[],"source":["# Load model\n","gg_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/byt5-small\")\n","tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")\n","\n","model = CPU_Unpickler(open('./byt5.pkl', 'rb')).load()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i0kVsVO96Uqj"},"outputs":[],"source":["from transformers import GenerationConfig\n","gen_conf = GenerationConfig.from_model_config(gg_model.config)\n","gen_conf.cache_implementation='dynamic'\n","gen_conf.output_logits = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EMHJ8hJb6Uqj"},"outputs":[],"source":["tst = interference('Nhạc hay lắm',gen_conf)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EehubC8L6Uqk","outputId":"dbc7372f-7eec-4b4d-8eda-74de008a96f4"},"outputs":[{"data":{"text/plain":["[('Song', 'NULL', 'positive', 'Nhạc hay lắm')]"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["tst"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"tags":[],"id":"Yc5N-V8w6Uqm","outputId":"62868fb6-ee8d-4c23-9969-0d38b958d674"},"outputs":[{"name":"stdout","output_type":"stream","text":["Showing live view refreshed every 10 seconds\n","Seconds passed: 0\n","break\n","Live view ended...\n"]}],"source":["for x in range(0, 2000):\n","    try:\n","        print(\"Showing live view refreshed every 10 seconds\")\n","        print(f\"Seconds passed: {x*10}\")\n","\n","        data_fixed = []\n","        results = []\n","        at = []\n","        ac = []\n","        ot = []\n","        sp = []\n","        comments = []\n","\n","        # Lấy data từ streamRaw\n","        data_pandas = streamRawDf.select('value').toPandas()\n","\n","        for i in range(0, streamRawDf.count()):\n","            data_fixed.append(data_pandas['value'][i].decode('unicode-escape').replace('\"', ''))\n","\n","        # Predict từ model\n","\n","        for i in range(0, len(data_fixed)):\n","            results.append(interference(data_fixed[i],gen_conf))\n","            lb = interference(data_fixed[i],gen_conf)\n","            for j in lb:\n","                    t1, t2 ,t3 ,t4 = j\n","                    ac.append(t1)\n","                    at.append(t2)\n","                    sp.append(t3)\n","                    ot.append(t4)\n","                    comments.append(data_fixed[i])\n","            t = pd.DataFrame({'Comments': comments, 'Aspect Category': ac, 'Aspect Term': at, 'Sentiment Polarity': sp, 'Opinion Term': ot})\n","            t.to_csv('data_result.csv', index=False)\n","            display(t)\n","        d = {'Text': data_fixed, 'Result': results}\n","        df_result = pd.DataFrame(d)\n","        display(df_result)\n","        a = df_result.copy()\n","        print('done')\n","\n","        clear_output(wait=True)\n","    except KeyboardInterrupt:\n","        print(\"break\")\n","        break\n","print(\"Live view ended...\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dIywUnsG6Uqo"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mtHwrY7W6Uqo"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}